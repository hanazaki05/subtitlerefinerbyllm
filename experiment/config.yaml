# Configuration for OpenAI SDK-based subtitle refinement
# This file controls all aspects of the subtitle processing pipeline

# API Settings
api:
  key_file: "../key"  # Path to file containing API key (relative to this config file)
  base_url: "https://api.openai.com/v1"
  timeout: 800  # API request timeout in seconds

# Main refinement model settings (GPT-5.* for subtitle correction)
main_model:
  name: "gpt-5-mini"
  #name: "gpt-4o"
  max_output_tokens: 27000
  reasoning_effort: "low"  # none, low, medium, high
  temperature: 1.0
  #temperature: 0.4

# Terminology extraction model settings (GPT-4o-mini for glossary extraction)
terminology_model:
  name: "gpt-4o-mini"
  max_output_tokens: 1800
  temperature: 0.45

# Token management
tokens:
  max_context_tokens: 160000
  memory_token_limit: 4000
  chunk_token_soft_limit: 85000

# Chunking strategy
chunking:
  pairs_per_chunk: null  # If set, use fixed pair count instead of token-based chunking

# Pricing (USD per 1000 tokens)
pricing:
  prompt_tokens: 0.03
  completion_tokens: 0.06

# Glossary settings
glossary:
  max_entries: 100
  policy: "lock"  # lock: user glossary is authoritative; learned terms cannot override
  terminology_min_confidence: 0.6  # Minimum confidence for extracted terminology entries

# User customization
user:
  prompt_path: "main_prompt.md"  # Path to main prompt template file (plan3.md strategy)

# Runtime options (can be overridden by CLI flags)
runtime:
  use_streaming: true  # Use OpenAI `chat/completion` stream API for real-time output (recommended)
  verbose: false  # Enable verbose output with detailed progress
  very_verbose: false  # Dump full API responses (requires verbose)
  debug_prompts: false  # Print system prompt/memory (requires very_verbose)
  stats_interval: 1.0  # Stats refresh interval in seconds (for verbose mode)
  dry_run: false  # Process limited pairs for testing
  max_chunks: null  # Maximum chunks to process (null = all)
